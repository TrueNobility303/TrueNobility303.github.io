---
title: '方差分析ANOVA'
toc: true
excerpt_separator: <!--more-->
tags:
  - 统计机器学习
---

方差分析（Analysis of Variance，ANOVA）研究自变量的不同水平对于因变量的影响，本文首先从线性回归模型出发，给出单因素ANOVA的基础理论，再从ANOVA本身模型的角度出发，分别推导单变量和双变量的ANOVA包括假设检验等。



<!--more-->



## From Regression to ANOVA

本节从线性回归模型的角度看ANOVA模型，作为对ANOVA的引入。

关于线性回归可以参见，[回归分析上](https://truenobility303.github.io/Rregression-First/) 和 [回归分析下](https://truenobility303.github.io/Rregression-Second/)



### Build the Model

模型采用如下假设，
$$
y_{ij} = \mu_i + \epsilon_{ij}, \epsilon_{ij} \sim \mathcal{N}(0,\sigma^2), (i =1..r,j =1..n_i) \\
$$

本质上如果将 $X$ 写成0-1矩阵的形式，也即将水平编码为哑变量（Dump Variables），可以转化为线性回归模型类似的形式，


$$
Y = X \mu + \epsilon, \epsilon \sim \mathcal{N}(0,\sigma^2)
$$
利用与线性回归完全相同的结论可以得到其最小二乘估计等价于极大似然估计，


$$
\begin{align}
\hat \mu_i &= (HY)_i = \bar Y_i  = \frac{\sum_{j=1}^{n_i} Y_i }{n_i}\\
\hat \sigma_{\text{MLE}} &= \frac{\text{SSE}}{n} =\frac{\sum_{ij} (Y_{ij} -\bar Y_i)^2 }{n}
\end{align}
$$


将方差的估计修正为无偏估计可以得到，


$$
\hat \sigma = \frac{\text{SSE}}{n-r}
$$


并且容易证明估计量的分布,


$$
\begin{align}
\hat \mu_i & \sim \mathcal{N}(\mu_i , \frac{\sigma^2} {n_i}) \\
(n-r) \hat \sigma^2 &\sim \sigma^2\chi^2(n-r)   \\
\hat \mu_i &\perp \hat \sigma
\end{align}
$$


其中两个估计量的独立性条件是构建假设检验的关键。



离差平方和和分解也与线性回归模型相同，


$$
\begin{align}
Y^TY &= Y^T H Y + Y^T(I-H)Y ,\text{With } H = X(X^TX)^{-1}X^T\\
Y^T(I - \frac{1}{n}ee^T) Y &= Y^T (H- \frac{1}{n}ee^T) Y + Y^T(I-H)Y\\
\text{SST} &= \text{SSR} + \text{SSE} ,\text{And } \text{SSR}  \perp \text{SST}\\
\sum_{ij} (Y_{ij} - \bar Y )^2 &= \sum_{ij}( \hat \mu_{i}- \bar {\hat \mu} )^2 + \sum_{ij} (Y_{ij} -\bar Y_i)^2 
\end{align}
$$
而且上式的期望可以利用迹运算等矩阵操作得到，



$$
\begin{align}
E[\text{SSE}] &= E[Y^T(I-H)Y] \\
&=E[\epsilon^T(I-H) \epsilon] \\
&= E[tr(\epsilon^T(I-H) \epsilon)] \\
&=tr((I-H) E[\epsilon \epsilon^T]) \\
&= tr(I-H) \sigma^2 I_n \\
&= \sigma^2 (n-r) \\
E[\text{SSR} ] &= E[Y^T (H- \frac{1}{n}ee^T) Y] \\
&= tr (PE[YY^T]),\text{With } P = H- \frac{1}{n}ee^T \\
&=(r-1) \sigma^2 + \sum_{ij}(\mu_{i}  -  \bar \mu)^2 \\
\end{align}
$$


实际上除了SST，SSR，SSE，在ANOVA中三者有其专有的含义更为直观的名字，详见下表，



| 名称     | 符号  | 自由度 | 线性回归对应   |
| -------- | ----- | ------ | -------------- |
| 总方差   | $ S $ | $ n-1$ | $ \text{SST} $ |
| 组间方差 | $S_A$ | $r-1$  | $ \text{SSR} $ |
| 组内方差 | $S_e$ | $n-r$  | $ \text{SSE} $ |



### Hypothesis Testing



使用最为普遍的Extra Sum of Square检验，零假设为零模型或者称为真模型（Reduced Model），而原本的模型称为全模型（Full Model），假设均值只亲存在线性相关性，表示为 $C \mu = d$,  在真模型下面的最小二乘估计为,


$$
\begin{align}
L &= (X \hat \mu - Y)^T(X \hat \mu - Y) + \lambda^T(C \hat \mu - d) \\
\frac{dL}{d \hat \mu} &= X^T(X \hat \mu - Y) + \lambda  C^T =0 \\
\hat \mu &=(X^TX)^{-1}(X^T Y - \lambda C^T) \\
C \hat \mu &= C(X^TX)^{-1}(X^T Y - \lambda C^T) = d \\
\lambda &=  (C(X^TX)^{-1} C^T)^{-1} (C(X^TX)^{-1}X^T Y -d) \\
\hat \mu &=(X^TX)^{-1}(X^T Y - C^T(C(X^TX)^{-1} C^T)^{-1} (C(X^TX)^{-1}X^T Y -d) ) \\
&=m - (X^TX)^{-1} C^T(C(X^TX)^{-1} C^T)^{-1} (Cm -d)  ,\\
\text{With } m &= (X^TX)^{-1}X^T Y
\end{align}
$$



假设在无约束的情况下求得的均值的估计为 $m$ , 计算两个模型的残差之间的关系，



$$
\begin{align}
\text{SSE}_R &= \Vert X \hat \mu  - Y \Vert_2^2 \\
&= \Vert X \hat \mu  - X m + Xm -Y \Vert_2^2 \\
&= \Vert X \hat \mu  - X m +(I-H)m\Vert_2^2, \text{With } H = X(X^TX)^{-1}X^T, HX =X \\
&=\Vert X \hat \mu  - X m \Vert_2^2 + \Vert (I-H) m \Vert_2^2 \\
&= \Vert X \hat \mu  - X m \Vert_2^2 + \text{SSE}_F \\
&= \Vert X(X^TX)^{-1} C^T(C(X^TX)^{-1} C^T)^{-1} (Cm -d) \Vert_2^2 + (Cm-d)^T +\text{SSE}_F  \\ 
&= (Cm-d)^T (C(X^TX)^{-1} C^T)^{-1}(Cm-d)+\text{SSE}_F \\
\text{SSE}_R - \text{SSE}_F &= (Cm-d)^T (C(X^TX)^{-1} C^T)^{-1}(Cm-d) \sim \chi^2(k) \\
\text{SSE}_R - \text{SSE}_F &=f(m) \perp \text{SSE}_F, \text{With } f \text{ a function}
\end{align}
$$



可以得到该检验统计量服从 $\chi^2$ 分布，自由度与限制矩阵 $C$ 的维度相关，最终可以得到检验统计量，


$$
\begin{align}
F = \frac{(\text{SSE}_R - \text{SSE}_F)/k}{\text{SSE}_F/(n-r)} \sim F_{k,n-r}
\end{align}
$$



考虑所有水平都相等的假设检验，也即零假设为 $\mu_1 = \mu_2 = ... =\mu_r$ :




$$
\begin{align}
F = \frac{(\text{SSE}_R - \text{SSE}_F)/(r-1)}{\text{SSE}_F/(n-r)} = \frac{S_A/(r-1)}{S_e/(n-r)} \sim F_{r-1,n-r}
\end{align}
$$



对于所有水平都不显著的假设检验，也即零假设为 $\mu_1 = \mu_2 = ... =\mu_r=0$ : 



$$
F = \frac{Y^T HY/r}{\text{SSE}_F/(n-r)} = \frac{\hat \mu^T \hat \mu /r}{\text{SSE}_F/(n-r)} \sim F_{r,n-r}
$$



对于单个系数的显著性检验，也即零假设为 $\mu_i = 0$ : 
$$
F = \frac{\hat \mu_i^2}{S^2 n_i} \sim F_{1,n-r}
$$



## One-Way ANOVA

从本节开始，将从ANOVA本身出发对其进行推导单因素方差分析（One-Way ANOVA），最初看起来可能略显麻烦，但这实际上是推广到双变量（Two-Way ANOVA）甚至于多变量方差分析的必由之路，证明过程中的技巧也非常具有学习价值和推广意义。

回归单变量方差分析的模型，假设一共有 $a$ 个水平，每个水平的均值为 $\mu_i$, 每个水平下面共有 $n_i$ 个观测样本，总共共有 $n$ 个样本，


$$
y_{ij} = \mu_i + \epsilon_{ij}, \epsilon_{ij} \sim \mathcal N(0,\sigma^2)
$$


### Parameter Estimate

类似于线性回归模型，最小二乘估计（OLS）等价于极大似然估计（MLE），尽管本质上是一样的，此处为了加深对ANOVA模型的理解，我们从头重新推导一遍，并且定义ANOVA模型中的符号，



先求解均值 $\mu_i $ 的估计，下面的符号可能不严谨，但由于结论较为显然，应该不影响理解，


$$
\begin{align}
\max_{\mu_i} L &=\max \prod_{ij} \frac{1}{\sqrt {2 \pi \sigma^2}} \exp(-\frac{(y_{ij} - \mu_i)^2}{2 \sigma^2}) ,\text{i.e. MLE }\\
&= \max_{\mu_i} [-\frac{n}{2} \log (2 \pi \sigma^2) - \frac{\sum_{ij}  \Vert y_{ij} - \mu_i \Vert_2^2}{2 \sigma^2}] \\
&= \min_{\mu_i} \sum_{ij}  \Vert y_{ij} - \mu_i \Vert_2^2 ,\text{i.e. OLS }\\
\frac{dL}{d \mu_i} &= \sum_j (y_{ij}- \mu_i) = 0 \\
\hat \mu_i &= \bar{y_{i .}}
\end{align}
$$


将其代入可以得到方差 $\sigma$ 的极大似然估计，


$$
\begin{align}
\max_{\sigma^2} L &=\max \prod_{ij} \frac{1}{\sqrt {2 \pi \sigma^2}} \exp(-\frac{(y_{ij} - \mu_i)^2}{2 \sigma^2}) \\
&= \max_{\sigma^2} [-\frac{n}{2} \log (2 \pi \sigma^2) - \frac{\sum_{ij}  \Vert y_{ij} - \mu_i \Vert_2^2}{2 \sigma^2}] \\
&= \min_{\sigma^2} [-\frac{n}{2} \log (2 \pi \sigma^2) - \frac{S_e}{2 \sigma^2}], \text{With } S_e = \sum_{ij}  \Vert y_{ij} - \bar y_{i .} \Vert_2^2 \\
\frac{d L}{d \sigma^2} &= -\frac{n}{2 \sigma^2} + \frac{S_e}{2 \sigma^4} = 0\\
\hat \sigma^2 &= \frac{S_e}{n}
\end{align}
$$


将方差的极大似然估计修正为无偏估计，


$$
\begin{align}
S_e &= \sum_{ij}  \Vert y_{ij} - \bar y_{i .} \Vert_2^2 \\
&= \sum_{ij} \Vert (\mu_{i} + \epsilon_{ij}) - (\mu_i + \bar \epsilon_{i .}) \Vert_2^2 \\
&= \sum_{ij} \Vert \epsilon_{ij} - \bar \epsilon_{i .} \Vert_2^2 \\
&= \sum_i \sigma^2 \chi^2(n_i - 1) \\
&= \sigma^2 \chi^2(n-a) \\
MSE &= \frac{S_e}{n-a}
\end{align}
$$


同理可以简单地验证Gauss-Markov定理，也即最佳线性无偏估计（BLUE）等价于上述估计，只需要对任意的BLUE估计分解其方差，


$$
\begin{align}
\hat \mu_i &= \bar y_{i.}+a_i^T y_{ij} \\
E[\hat \mu_i] &= 0 \Rightarrow a_i^T \mu_i = 0 \Rightarrow a_i^T e_i = 0\\
Var[\hat \mu_i] &= Var[\bar y_{i.}+a_i^T y_{ij}] \\
&=Var[\bar y_{i.}] + Var[a_i^T y_{ij}] + Cov[\bar y_{i.}, a_i^T y_{ij}] \\
&=Var[\bar y_{i.}] + Var[a_i^T y_{ij}] + Cov[\bar \epsilon_{i.}, a_i^T \epsilon_{ij}] \\

&=Var[\bar y_{i.}] + Var[a_i^T y_{ij}] \\
&\ge Var[\bar y_{i . }]
\end{align}
$$

### Hypothesis Testing



统计模型中重要的步骤是假设检验，ANOVA的假设检验首先依赖于平方和分解，


$$
\begin{align}
S_T &= \sum_{ij} \Vert y_{ij} - \bar y_{..} \Vert_2^2 \\
&= \sum_{ij} \Vert y_{ij} - \bar y_{i.} \Vert_2^2 + \sum_{ij} \Vert \bar y_{i.} - \bar y_{..} \Vert_2^2 \\
&=S_e + S_A 
\end{align}
$$


显然残差 $S_e$ 服从卡方分布，


$$
\begin{align}
S_A&= \sum_{ij} \Vert y_{ij} - \bar y_{i.} \Vert_2^2 \\
&= \sum_{ij} \Vert \epsilon_{ij} - \bar \epsilon_{i.} \Vert_2^2 \\
&=\sigma^2\chi^2(n-a) 
\end{align}
$$


在ANOVA中另外一个重要的指标称为主效应，对水平的检验通常化为对主效应的检验，其定义如下，


$$
\begin{align}
S_A &= \sum_{ij} \Vert \bar y_{i.} - \bar y_{..} \Vert_2^2 \\
&=\sum_{ij} \Vert (\mu_i + \bar \epsilon_{i.} )-(\bar \mu_{.} + \bar \epsilon_{..} ) \Vert_2^2 \\
&=\sum_{ij} \Vert a_i + \bar \epsilon_{i.} - \bar \epsilon_{..} \Vert_2^2 ,\text{Let } \mu_i = a_i + \bar \mu_{.}\\
&= \sum_i n_i \Vert a_i + \bar \epsilon_{i.} - \bar \epsilon_{..} \Vert_2^2 \\ 
\end{align}
$$


由于在实际应用中我们通常希望检验每个水平是否影响相同，也即检验 $a_i = 0$,  我们希望知道零假设下的分布，使用一个高维的投影阵可以直观理解，


$$
\begin{align}
S_A &= \sum_{ij} \Vert  \bar \epsilon_{i.} - \bar \epsilon_{..} \Vert_2^2 , \text{Under } H_0\\ 
&= \epsilon^T (P_1 - P_2) \epsilon ,\text{Let } \epsilon = [\epsilon_{ij}], P_1 \epsilon = \bar \epsilon_{i.}, P_2 \epsilon = \bar \epsilon_{..} \\
&\sim \sigma^2 \chi^2(a-1) , \text{With } tr(P_1) = a, tr(P_2) = 1 
\end{align}
$$


并且独立性条件可以得到满足，从推导过程中可以看出，该独立性条件甚至不需要依赖于 $H_0$


$$
\begin{align}
S_e &= Y^T(I- P_3) Y  = \Vert (I-P_1) Y \Vert_2^2\\
S_A &= \epsilon^T (P_1 - P_3 )\epsilon = \Vert (P_1 - P_3) \epsilon \Vert_2^2 \\
Cov[(I-P_1) Y,(P_1 - P_3) \epsilon] &= Cov[(I-P_1) \epsilon,(P_1 - P_3) \epsilon] \\
&= (I -P_1)(P_1 - P_3) \sigma^2 \\
&=0 \\
S_A &\perp S_e
\end{align}
$$


因此为了检验主效应为0，构造F统计量，


$$
\begin{align}
F &= \frac{MSA}{MSE} =  \frac{S_A / (a-1)}{S_e /(n-a)} \sim  F_{a-1,n-a}
\end{align}
$$


在显著性水平 $\alpha$ 下，若统计量大于 $F_{a-1,n-a}(\alpha)$ 则拒绝 $H_0$. 



## Two-Way ANOVA

本节考虑双变量方差分析的情况，此时一共有 $A,B$ 两种水平，每个水平个数分别为 $a,b $, 由于双变量的方差分析相较于单变量情况更为复杂，需要假设每个水平下面的观测样本个数均为 $n$ , 模型假设为，



$$
\begin{align}
y_{ijk} &= \mu_{ij} + \epsilon_{ijk} ,\epsilon_{ijk} \sim \mathcal{N}(0,\sigma^2) \\

\end{align}
$$



此时的主效应包括了两个水平的效应已经两个水平的交互效应，



$$
\begin{align}
a_i &= \bar \mu_{i.} - \bar \mu_{..} \\
b_j &= \mu_{.j} - \bar \mu_{..} \\
\gamma_{ij} &= \mu_{ij} - \bar \mu_{i.} - \bar \mu_{.j} + \bar \mu_{..} \\
\mu_{ij} &= a_i + b_j + \gamma_{ij} + \bar \mu_{..}
\end{align}
$$



可以首先简单地观察主效应的一些性质，此类性质对于后续的平方和分解和假设检验等将扮演者重要作用，


$$
\begin{align}
\sum_i a_i &= \sum_j b_j =\sum_i \gamma_{ij} = \sum_j \gamma_{ij} = 0 \\
\end{align}
$$


嵌入对于均值的估计量就可以得到对主效应的估计，且对主效应的估计仍然满足上述求和为0的性质，


$$
\begin{align}
\hat a_i &= \bar y_{i..} - \bar y_{...} \\
\hat b_i &= \bar y_{.j.} - \bar y_{...} \\
\hat \gamma_i &= \bar y_{ij.} - \bar y_{i..} - \bar y_{.j.} + \bar y_{...} \\
\bar y_{ij.} &= \hat a_i + \hat b_i + \hat \gamma_{ij} + \bar y_{...} \\
\sum_i \hat a_i &= \sum_j \hat b_j =\sum_i \hat \gamma_{ij} = \sum_j \hat \gamma_{ij} = 0 \\
\end{align}
$$


完全相同地，模型的极大似然估计和最小二乘估计等价，据此可以得到，



$$
\begin{align}
\hat \mu_{ij} &= \bar y_{ij.} \\
S_e &= \sum_{ijk} (y_{ijk} - \bar y_{ij.} )^2 \\
&= \sum_{ijk} (\epsilon_{ijk} - \bar \epsilon_{ij.})^2 \\
&\sim \sum_{ij} \sigma^2 \chi^2(n-1) \\
&\sim \sigma^2 \chi^2(ab(n-1)) \\
\hat \sigma^2 &= MSE = \frac{S_e}{ab(n-1)} \\
\end{align}
$$



### Testing With Interaction



对于双因素的方差分析模型，我们关心两个不同因素的交互效应是否存在，在不存在交互效应的情况下模型变得简单，也即自变量两个水平上对因变量的影响可以进行简单的线性叠加。



仍然考虑平方和分解，


$$
\begin{align}
S_T &= \sum_{ijk} \Vert y_{ijk} - \bar y_{...} \Vert_2^2 \\
&= \sum_{ijk} \Vert y_{ijk} - \bar y_{ij.} \Vert_2^2 + \sum_{ijk} \Vert \bar y_{ij.} - \bar y_{...} \Vert_2^2 \\
&= S_e +  S_{AB} \\

\end{align}
$$


进一步和模型有关的项 $S_{AB}$ 可以被分解为两个水平及其交互项组成的平方和，


$$
\begin{align}
S_{AB} &= \sum_{ijk} \Vert \bar y_{ij.} - \bar y_{...} \Vert_2^2 \\ 
&= \sum_{ijk} \Vert \hat a_i + \hat b_j + \hat \gamma_{ij} \Vert_2^2 \\
&= \sum_{ijk} \Vert \hat a_i + \hat b_j \Vert_2^2 + \sum_{ijk} \Vert \hat \gamma_{ij} \Vert_2^2, \text{By } \sum_i \hat \gamma_{ij} = \sum_j \hat \gamma_{ij} = 0 \\
&= \sum_{ijk} \Vert \hat a_i \Vert_2^2 + \sum_{ijk} \Vert \hat b_j \Vert_2^2 + \sum_{ijk} \Vert \hat \gamma_{ij} \Vert_2^2, \text{By } \sum_i \hat a_i = \sum_j \hat b_j = 0 \\
&=S_A' +S_B' + S_{AB}'  \\
\end{align}
$$


考虑主效应的估计量和其真实值之间的关系，


$$
\begin{align}
\hat a_i &=  \bar y_{i..} - \bar y_{...} \\
&=a_i + \bar \epsilon_{i..} - \bar \epsilon_{...} \\
\hat b_j &= \bar y_{.j.} - \bar y_{...} \\
&= b_j + \bar \epsilon_{.j.} - \bar \epsilon_{...} \\
\hat \gamma_{ij} &= \bar y_{ij.} - \bar y_{i..} - \bar y_{.j.} + \bar y_{...} \\
&=\gamma_{ij} + \bar \epsilon_{ij.} - \bar \epsilon_{i..} - \bar \epsilon_{.j.} + \bar \epsilon_{...} 
\end{align}
$$


同样定义投影矩阵 $P_1,P_2,P_3$ 同时将所有的误差项拼接成一个向量 $\epsilon = [\epsilon_{ijk}]$, 容易验证上述矩阵结合起来也可以得到投影阵，


$$
\begin{align}
\text{Define } P_1 \epsilon &= \bar \epsilon_{i..} , P_2 \epsilon = \bar \epsilon_{.j.}, P_3 \epsilon = \bar \epsilon_{...},P_4 \epsilon = \bar \epsilon_{ij.} \\
\hat a_i &= Y^T(P_1 - P_3) Y \\
&= a_i + \epsilon^T(P_1 - P_3) \epsilon \\
\hat b_j &= Y^T(P_2 - P_3) Y \\
&= b_j + \epsilon^T(P_2 - P_3) \epsilon \\
\hat \gamma_{ij} &= Y^T(P_4 - P_1 -P_2 +P_3) Y^T \\
&=\gamma_{ij} + \epsilon^T(P_4 - P_1 -P_2 + P_3) \epsilon \\
S_e &=\epsilon^T(I - P_4) \epsilon \\
\end{align}
$$


对于组合后投影阵的验证是简单的，由于对称性显然满足，只需要验证幂等性，


$$
\begin{align}
(P_1 -P_3)^2 &=P_1 - 2P_1 P_3 +P_3 \\
&= P_1 - P_3 \\
(P_2 -P_3)^2 &=P_2 - 2P_2 P_3 +P_3 \\
&= P_2 - P_3 \\
(P_4 - P_1 -P_2 +P_3)^2 &= P_4 +P_1 +P_2 +P_3 - 2P_1P_4 -2P_2P_4 - 2P_1P_3 - 2P_2P_3 +2P_1P_2 +2P_3P_4 \\
&=P_4+P_1 +P_2 +P_3 -2P_1 -2P_2 -2P_3-2P_3 +  2P_3 +2P_3 \\
&=P_4-P_1 - P_2 +P_3 \\
\text{Where }tr(P_1) &= a, tr(P_2) = b,tr(P_3) = 1, tr(P_4) = ab
\end{align}
$$


据此从投影阵的秩可以直接得到自由度，此处暂略，此处直接推导零假设下的分布情况，从分布中也可以看出自由度，


$$
\begin{align}
\text{If } a_i &= 0 ,\text{Then } \hat a_i = \epsilon^T(P_1 - P_3) \epsilon \sim \sigma^2 \chi^2(a-1) \\
\text{If } b_j &= 0, \text{Then } \hat b_j = \epsilon^T(P_2 - P_3) \epsilon \sim \sigma^2 \chi^2(b-1) \\
\text{If } \gamma_{ij} &=0 ,\text{Then } \hat \gamma_{ij} =\epsilon^T(P_4 - P_1 - P_2 +P_3) \epsilon \sim \sigma^2 \chi^2((a-1)(b-1))
\end{align}
$$


并且根据投影阵的关系可以计算协方差，得到不依赖于 $H_0$ 的一系列独立性的条件，


$$
\begin{align}
(P_1 - P_3) (P_4 - P_1 - P_2 +P_3) &= O\\ 
(P_2 - P_3) (P_4 - P_1 - P_2 +P_3) &= O \\
(I- P_4)(P_1 - P_3) &= O \\
(I- P_4)(P_2 - P_3) &= O \\
(I-P_4)(P_4 - P_1 - P_2 +P_3) &= O \\
\end{align}
$$


因而可以得到以下的独立性，


$$
S_e \perp S_A' , S_e  \perp S_B' , S_e  \perp S_{AB}' ,
S_A'  \perp S_{AB}' ,
S_B'  \perp S_{AB}'
$$




对于交互项的主效应的假设检验，我们希望检验 $\gamma_{ij} = 0$ 是否成立，使用F统计量，


$$
\begin{align}
F = \frac{MSAB}{MSE} = \frac{S_{AB}'/((a-1)(b-1))}{S_e/ (ab(n-1))} \sim F_{(a-1)(b-1),ab(n-1)}
\end{align}
$$


对于 $a_i = 0 $  或者 $b_j = 0 $ 的检验基本上与单因素方差分析保持一致，


$$
\begin{align}
F_a &= \frac{MSA}{MSE} = \frac{S_A' / (a-1)}{S_e/(ab(n-1))} \sim F_{a-1,ab(n-1)} \\
F_b &= \frac{MSB}{MSE} = \frac{S_B' / (b-1)}{S_e/(ab(n-1))} \sim F_{b-1,ab(n-1)} \\
\end{align}
$$


### Testing Without Interaction



本节考虑一种特殊的情况，假设已知模型没有交互效应的存在，此时希望进一步检验某个水平是否有显著的影响，在该模型之下ANOVA又将出现不同，主要的区别在于残差项 $S_e$ 的估计上面。由于已知没有交互项的额外的信息，对于残差项的估计会变得更加准确一些。



使用投影阵的方式回顾平方和分解，可以更加深刻地理解其本质，


$$
\begin{align}
Y^T(I-P_3) Y &= \epsilon^T(I- P_4) \epsilon+ Y^T(P_4 - P_3 )  Y \\
&= \epsilon^T(I- P_4) \epsilon + Y^T(P_1 - P_3 )  Y +Y^T(P_2 - P_3 ) Y +Y^T(P_4 - P_1 - P_2 +P_3 )  Y \\
S_T &= S_e + S_{AB} \\
&= S_e +S_A' +S_B' +S_{AB}' \\
\end{align}
$$

其中化简基于的核心来自于，
$$
\begin{align}
y_{ijk} &= \mu_{ij} + \epsilon_{ijk} \\
y_{ijk} - \bar y_{ij.} &= \epsilon_{ijk} - \bar \epsilon_{ij.}
\end{align}
$$


而如果已知没有交互效应的存在，此时与交互效应相关的一项也变成残差项，应该将其吸收进 $S_e$ 中，

$$
\begin{align}
Y^T(I-P_3) Y &= \epsilon^T(I- P_4) \epsilon + Y^T(P_4 - P_3 ) Y\\
&= \epsilon^T(I- P_1 -P_2 +P_3) \epsilon + Y^T(P_1 - P_3 )  Y +Y^T(P_2 - P_3 )  Y  \\
S_T &= S_e +S_{AB} \\
&=S_e' +S_A' +S_B' 
\end{align}
$$



而化简的核心来自于没有交互项的时候，


$$
\begin{align}
y_{ijk} &= a_i+ b_j + \bar \mu_{..} + \epsilon_{ijk} \\
\bar y_{i..} &= a_i + \bar \mu_{..} + \bar \epsilon_{i..} \\
\bar y_{.j.} &= b_j +\bar \mu_{..} + \bar \epsilon_{.j.} \\
\bar y_{...} &= \bar \mu_{..} + \bar \epsilon_{...}\\ 
y_{ijk} - \bar y_{i..} - \bar y_{.j.} + \bar y_{...} &= \epsilon_{ijk} - \bar \epsilon_{i..} - \bar \epsilon_{.j.} + \epsilon_{...}\\
\end{align}
$$


为了进行假设检验，思考独立性条件，实际上独立性条件已经在之前得到了证明，

$$
\begin{align}
\text{By }S_e' &= S_e + S_{AB}' ,S_A' \perp S_e, S_A' \perp S_{AB}' \\
\text{Then } S_A' & \perp S_e'
\end{align}
$$



据此就得到了对于 $a_i = 0 $  或者 $b_j = 0 $ 的检验， 

$$
\begin{align}
F_a &= \frac{MSA}{MSE} = \frac{S_A' / (a-1)}{S_e'/(ab(n-1) + (a-1)(b-1))} \sim F_{a-1,ab(n-1)+(a-1)(b-1)} \\
F_b &= \frac{MSB}{MSE} = \frac{S_B' / (b-1)}{S_e/(ab(n-1)+(a-1)(b-1))} \sim F_{b-1,ab(n-1)+(a-1)(b-1)} \\
\end{align}
$$


上面从较高维度得到了 $S_e'$ 的估计，但在实际应用中我们可能需要的是其显示表达式，因此仍然将其写出来，


$$
\begin{align}
S_T &= \sum_{ijk} \Vert y_{ijk} - \bar y_{...} \Vert_2^2 \\
&= \sum_{ijk} \Vert (y_{ijk} - \bar y_{i..} - \bar y_{.j.} + \bar y_{...}) +(\bar y_{i..}- \bar y_{...}) + (\bar y_{.j.} - \bar y_{...}) \Vert_2^2 \\
&= \sum_{ijk} \Vert y_{ijk} - \bar y_{i..} - \bar y_{.j.} + \bar y_{...} \Vert_2^2 + \sum_{ijk} \Vert \bar y_{i..}- \bar y_{...} \Vert_2^2 + \sum_{ijk} \Vert \bar y_{.j.} - \bar y_{...} \Vert_2^2 \\
&=S_e' +S_A' +S_B'
\end{align}
$$


### Confidence Interval



对于参数的区间估计，可以得到其对应的置信区间，实际上并不复杂，利用估计量的分布即可，


$$
\begin{align}
\hat \mu_{ij} &\sim \mathcal{N} (\mu_{ij}, \frac{\sigma^2}{n} )
\end{align}
$$


而由于我们实际上并不能得知模型的真实方差 $\sigma^2$, 需要用方差的一个无偏估计进行嵌入。

嵌入的时候需要根据模型是否有交互效应选择不同的估计量，因为在不同情况下对方差的估计是不同的。

不同的模型下对方差的估计量不相同，将导致其自由度和自由度等都出现差异，但本质并无区别。





