---
title: '熵'
toc: true
excerpt_separator: <!--more-->
tags:
  - 统计机器学习
---



熵相关的内容，是机器学习中重要的一部分，例如决策树的特征选择等。包括KL散度、信息熵、条件熵、互信息等。



<!--more-->





## Relative entropy(KL Divergence)

相对熵，或者称为KL散度，用于刻画两个分布之间的距离，且满足：


$$
\begin{align}
KL(p \Vert q) &= E_p \log \frac{p}{q} \\
&= -E_p \log \frac{p}{q} \\
&\ge -\log E_p\frac{q}{p} \\
&=0, \text{Iff } p=q
\end{align}
$$
如果是离散型随机变量，也可以类似地定义KL散度作为距离的度量，此时其非负性的证明需要用到数列不等式，可能稍显繁琐一点，此处暂略。



## Entropy

熵刻画了一个分布的不确定性，熵越大不确定性越大，定义为：


$$
\begin{align}
H(p) &= -E_p \log p \\
H(X) &= -\int p(x) \log p(x) dx  
\end{align}
$$


## Joint Entropy

联合熵为联合分布的熵，


$$
\begin{align}
H(X,Y) &= -\int p(x,y) \log p(x,y) dx dy \\
\end{align}
$$


## Conditional Entropy

条件熵刻画了给定一个随机变量之后另一个随机变量的熵，


$$
\begin{align}
H(X \vert Y) &= H(X,Y) - H(Y) \\
&= -\int p(x,y) \log p(x,y) dx dy + \int p(y) \log p(y) dy \\ 
&= -\int p(x,y) \log p(x,y) dx dy + \int p(x,y) \log p(y) dxdy \\
&=-\int p(x,y) \log \frac{p(x,y)}{p(y)} dxdy \\
&= -\int p(x,y) \log p(x \vert y) dxdy 
\end{align}
$$


可见条件熵相当于关于条件概率的熵的期望。



## Cross Information(Infomation Gain)

互信息为两个随机变量之间相关的信息量，也可以用信息增益定义：


$$
\begin{align}
I(X,Y) &= H(X) - H(X \vert Y) \\
&= -\int p(x) \log p(x) dx +\int p(x,y) \log p(x \vert y) dxdy  \\
&= -\int p(x,y) \log \frac{p(x)}{p(x \vert y)} dxdy \\
&= -\int p(x,y) \log \frac{p(x) p(y)}{p(x,y)} dxdy \\
&= KL(p(x,y) \Vert p(x)p(y)) 
\end{align}
$$


可见互信息等价于联合分布关于边缘分布的相对熵（KL散度），因此互信息一定是非负的。



## Relationship

熵、条件熵、联合熵、互信息之间的关系可以用类似Venn图的方法清晰地表述，如果讲随机变量$X,Y$分别用集合$A,B$表示，则：


$$
\begin{align}
H(X) &= P(A),H(Y) = P(B) \\ 
H(X \vert Y) &= P(A B^C), H(Y \vert X) = P(B A^C) \\
I(X,Y) &= P(A \cap B),H(X,Y) = P(A \cup B)
\end{align}
$$


因此其之间的关系也可以很简单地看出来。

