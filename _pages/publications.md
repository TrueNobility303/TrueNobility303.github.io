---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

My full publications list is on [Google Scholar](https://scholar.google.com/citations?user=ynGzhugAAAAJ&hl=en&oi=ao). (Due to the possibility of making minor updates to the published version on arXiv, please refer to the following links on my homepage for the latest version of my publications if you are interested in them)

Note that <sup>1</sup> denotes equal contributions.

 <h2> Recent Preprints </h2>

<ul>
<font size="3">
<li><p>  <b>Lesi Chen</b>, Chengchang Liu, Luo Luo, Jingzhao Zhang,
<i> Computationally Faster Newton Methods by Lazy Evaluations </i>,
 arXiv preprint, 2025.
 <a href="https://arxiv.org/abs/2501.17488">[paper]  </a>
</p></li>
<li><p> <b>Lesi Chen<sup>1</sup></b>, Yaohua Ma<sup>1</sup>, Jingzhao Zhang,
 <i> Near-Optimal Nonconvex-Strongly-Convex Bilevel Optimization with Fully First-Order Oracles </i>,
 arXiv preprint, 2023.
  <a href="https://arxiv.org/abs/2306.14853">[paper]  </a>
 </p></li>
</font>
</ul>

<h2> Selected Publications </h2>

<ol>
<font size="3">  
<li><p> <b>Lesi Chen<sup>1</sup></b>, Chengchang Liu<sup>1</sup>, Jingzhao Zhang,  <i> Second-Order Min-Max Optimization with Lazy Hessians</i>, in International Conference on  Learning Representations (ICLR), 2025 <b> (Oral) </b>. 
 <a href="https://arxiv.org/pdf/2410.09568">[paper]</a> 
 </p></li>
<li><p> <b>Lesi Chen</b> and Luo Luo, <i> Near-Optimal Algorithms for Making the Gradient Small in Stochastic Minimax Optimization</i>, Journal of Machine Learning (JMLR), 2024.
  <a href="https://arxiv.org/abs/2208.05925">[paper]</a>
</p></li> 
<li><p> Huaqing Zhang<sup>1</sup>, <b>Lesi Chen<sup>1</sup></b>, Jing Xu, and Jingzhao Zhang, <i>
 Functionally Constrained Algorithm Solves Convex Simple Bilevel Problems</i>, in Conference on Neural Information Processing Systems (NeurIPS), 2024.
 <a href="https://arxiv.org/abs/2409.06530">[paper]  </a>
 </p></li>
<!--  <li><p>
Decentralized Convex Finite-Sum Optimization with Better Dependence on Condition Numbers. <br />
Yuxing Liu, <b>Lesi Chen</b>,  and Luo Luo. <a href="https://openreview.net/pdf?id=LLdeUPOUXk">[ICML 2024] </a> 
</p></li>
<li><p> Communication Efficient Distributed Newton Method with Fast Convergence Rates. <br />
 Chengchang Liu, <b>Lesi Chen</b>, Luo Luo, and John C.S. Lui. <a href="https://arxiv.org/abs/2305.17945">[SIGKDD 2023] </a>
</p> </li> -->
<li><p> <b>Lesi Chen<sup>1</sup></b>, Jing Xu<sup>1</sup>, Jingzhao Zhang, <i> On Finding Small Hyper-Gradients in Bilevel Optimization: Hardness Results and Improved Analysis</i>,
 in Conference on Learning Theory (COLT), 2024.
  <a href="https://arxiv.org/abs/2301.00712">[paper] </a>
</p></li>
<!--  <li><p> An Efficient Stochastic Algorithm for Decentralized Nonconvex-Strongly-Concave Minimax Optimization. <br />
<b>Lesi Chen</b>, Haishan Ye, and Luo Luo. <a href="https://arxiv.org/abs/2212.02387">[AISTATS 2024] </a>
</p> </li> -->
 <li><p> <b>Lesi Chen</b>, Jing Xu,  Luo Luo, <i> Faster Gradient-Free Algorithms for Nonsmooth Nonconvex Stochastic Optimization</i>,
 in International Conference on Machine Learning (ICML), 2023. 
  <a href="https://arxiv.org/abs/2301.06428"> [paper] </a>
 </p> </li>
<li><p>  <b>Lesi Chen</b>, Boyuan Yao, Luo Luo, <i> Faster Stochastic Algorithms for Minimax Optimization under Polyak-≈Åojasiewicz Condition</i>, 
 in Conference on Neural Information Processing Systems (NeurIPS), 2022.
  <a href="https://arxiv.org/abs/2307.15868"> [paper] </a>
 </p> </li> 
</font>
</ol>

  
  
