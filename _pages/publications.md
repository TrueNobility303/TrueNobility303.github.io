---
layout: archive
title: "Research"
permalink: /publications/
author_profile: true
---

I have a broad interest in modern optimization theory (inspired by theoretical computer science). Currently, I am primarily focusing on the complexity of multi-agent optimization (MAOPT). In my representative work ‚≠ê, I have collaborated with my excellent coauthors to solve many fundamental problems in MAOPT, including the complexity of first-order bilevel optimization [1,3,8] and high-order min(-max) optimization [2,4,5].

<img src="/images/research/Minimax.jpg" style="max-width: 100%; height: auto;">


<h2> Featured Publications </h2>

<sup>1</sup> indicates co-first-authors, <sup>#</sup> indicates undergraduates supervised by me and Jingzhao.
    
<ol class="custom-ol">
<font size="3">  
<li><p> 
<b>Lesi Chen</b>, Junru Li, and Jingzhao Zhang, <i> Faster Gradient Methods for Highly-smooth Stochastic Bilevel Optimization</i>, arXiv preprint. <a href="https://arxiv.org/abs/2509.02937">[arXiv 2025]</a> ‚≠ê
</p>
</li>    
<li><p> <b>Lesi Chen</b>, Chengchang Liu, Luo Luo, and Jingzhao Zhang,  <i> Solving Convex-Concave Problems with $\tilde{\mathcal{O}}(\epsilon^{-4/7})$ Second-Order Oracle Complexity</i>, in Conference on Learning Theory. <a href="http://arxiv.org/abs/2506.08362">[COLT 2025]</a> ‚≠ê <br>  <b>Best Student Paper Award 
</b> üèÜ
</p></li> 
<li><p> <b>Lesi Chen<sup>1</sup></b>, Yaohua Ma<sup>1#</sup>, and Jingzhao Zhang,
 <i> Near-Optimal Nonconvex-Strongly-Convex Bilevel Optimization with Fully First-Order Oracles </i>, Journal of Machine Learning Research, 1-56. 
  <a href="https://arxiv.org/abs/2306.14853">[JMLR 2025]</a> ‚≠ê
 </p></li>
<li><p>  <b>Lesi Chen</b>, Chengchang Liu, Luo Luo, and Jingzhao Zhang,
<i> Computationally Faster Newton Methods by Lazy Evaluations </i>,
 arXiv preprint (extended from ICLR 2025 below).
 <a href="https://arxiv.org/abs/2501.17488">[arXiv 2025]</a> ‚≠ê
</p></li>
<li><p> <b>Lesi Chen<sup>1</sup></b>, Chengchang Liu<sup>1</sup>, and Jingzhao Zhang,  <i> Second-Order Min-Max Optimization with Lazy Hessians</i>, in International Conference on  Learning Representations. <b>(Oral) top 1.8% </b>  <a href="https://arxiv.org/pdf/2410.09568">[ICLR 2025]</a> 
</p></li>
<li><p> <b>Lesi Chen</b> and Luo Luo, <i> Near-Optimal Algorithms for Making the Gradient Small in Stochastic Minimax Optimization</i>, Journal of Machine Learning Research, 1-44. 
  <a href="https://arxiv.org/abs/2208.05925">[JMLR 2024]</a>
</p></li> 
<li><p> Huaqing Zhang<sup>1#</sup>, <b>Lesi Chen<sup>1</sup></b>, Jing Xu, and Jingzhao Zhang, <i>
 Functionally Constrained Algorithm Solves Convex Simple Bilevel Problems</i>, in Conference on Neural Information Processing Systems. <br>
 <a href="https://arxiv.org/abs/2409.06530">[NeurIPS 2024]</a>
 </p></li>
<!-- 
<li><p> Communication Efficient Distributed Newton Method with Fast Convergence Rates. <br />
 Chengchang Liu, <b>Lesi Chen</b>, Luo Luo, and John C.S. Lui. <a href="https://arxiv.org/abs/2305.17945">[SIGKDD 2023] </a>
</p> </li> -->
<li><p> <b>Lesi Chen<sup>1</sup></b>, Jing Xu<sup>1</sup>, and Jingzhao Zhang, <i> On Finding Small Hyper-Gradients in Bilevel Optimization: Hardness Results and Improved Analysis</i>,
 in Conference on Learning Theory. 
<a href="https://arxiv.org/abs/2301.00712">[COLT 2024]</a> ‚≠ê
</p></li>
<!--  <li><p>
Yuxing Liu, <b>Lesi Chen</b>,  and Luo Luo, <i>
Decentralized Convex Finite-Sum Optimization with Better Dependence on Condition Numbers</i>.
 <a href="https://openreview.net/pdf?id=LLdeUPOUXk">[ICML 2024] </a> 
</p></li>
<li><p> <b>Lesi Chen</b>, Haishan Ye, and Luo Luo, <i>An Efficient Stochastic Algorithm for Decentralized Nonconvex-Strongly-Concave Minimax Optimization</i>,
in International Conference on Artificial Intelligence and Statistics. <a href="https://arxiv.org/abs/2212.02387">[AISTATS 2024] </a>
</p> </li>
<li><p> Luo Luo, Yunyan Bai, <b>Lesi Chen</b>, Yuxing Liu, Haishan Ye,
<i> On the Complexity of Decentralized Smooth Nonconvex Finite-Sum Optimization </i>, arXiv preprint.
 <a href="https://arxiv.org/abs/2210.13931">[arXiv 2022]</a> ‚≠ê
</p> </li> -->
<li><p> <b>Lesi Chen</b>, Jing Xu, and Luo Luo, <i> Faster Gradient-Free Algorithms for Nonsmooth Nonconvex Stochastic Optimization</i>,
 in International Conference on Machine Learning. 
  <a href="https://arxiv.org/abs/2301.06428"> [ICML 2023]</a>
 </p> </li>
<li><p>  <b>Lesi Chen</b>, Boyuan Yao, and Luo Luo, <i> Faster Stochastic Algorithms for Minimax Optimization under Polyak-≈Åojasiewicz Condition</i>, 
 in Conference on Neural Information Processing Systems.
  <a href="https://arxiv.org/abs/2307.15868"> [NeurIPS 2022]</a>
 </p> </li>
</font>
</ol>

See [Google Scholar](https://scholar.google.com/citations?user=ynGzhugAAAAJ&hl=en&oi=ao) for a complete list.
  
  
